{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "pfrl_quickstart.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kGOKkjslQAb"
      },
      "source": [
        "# PRFL Quickstart Guid\n",
        "\n",
        "[PFRL Quickstart Guid][quick]の写経になります。\n",
        "\n",
        "[quick]: https://github.com/pfnet/pfrl/blob/master/examples/quickstart/quickstart.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DE7vJ2xTl_Gr"
      },
      "source": [
        "# Install packages\n",
        "\n",
        "PFRLを利用するためのpipインストールと、OpenAI GYMのrenderをcolab上で利用するために必要なパッケージを追加しています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qke6sZSR0HLC"
      },
      "source": [
        "# colab上でOpen AI Gymのrendoerを実行するための準備\n",
        "# https://qiita.com/ymd_h/items/c393797deb72e1779269\n",
        "# https://colab.research.google.com/drive/1flu31ulJlgiRL1dnN2ir8wGh9p7Zij2t#scrollTo=8nj5sjsk15IT\n",
        "!pip install gym pyvirtualdisplay > /dev/null 2>&1\n",
        "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmg2EQtLmBA_"
      },
      "source": [
        "!pip install pfrl 1>/dev/null"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yb_Bbk8FlmR1"
      },
      "source": [
        "# Preset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f1ibz4zynr-c"
      },
      "source": [
        "# default packages\n",
        "import base64\n",
        "import io\n",
        "import logging\n",
        "import pathlib\n",
        "from typing import Generator, Tuple"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LZIyzsPklp7Q"
      },
      "source": [
        "# third party packaegs\n",
        "import gym\n",
        "import gym.wrappers as gwrappers\n",
        "import IPython.display as display\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pfrl\n",
        "import pyvirtualdisplay as pvd\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ip0pOw4dpvGU"
      },
      "source": [
        "renderを実施するために仮想ディスプレイを設定します。\n",
        "このインスタンスは、以降では直接利用しないですが、関数内で実施すると適切に動作しなかったため、グローバル変数として設定しています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fu7tL-dzzE8_"
      },
      "source": [
        "DISPLAY = pvd.Display(visible=0, size=(1400, 900))\n",
        "DISPLAY.start()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rLyeazVTl2Fu"
      },
      "source": [
        "# logger\n",
        "_logger = logging.getLogger(__name__)\n",
        "logging.basicConfig(level=logging.INFO)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxtjO6j_qboQ"
      },
      "source": [
        "# CartPole-v0\n",
        "\n",
        "gymのenvは下記のメソッドを持つ。\n",
        "\n",
        "- `env.reset`: 環境を初期化し最初の観測値を返す。\n",
        "- `env.step`: アクションを実行し、次の状態へ遷移する。その際に下記の値を返す。\n",
        "    - 次の観測値\n",
        "    - 報酬\n",
        "    - 現在の状態が終了しているかどかのboolean\n",
        "    - その他の情報\n",
        "- `env.render`: 現在の状態をレンダリングする。(オプション)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "osE1Erx-4SzL"
      },
      "source": [
        "def show_video(filepath: pathlib.Path) -> None:\n",
        "    \"\"\"mp4形式の動画をcolab上で表示します.\"\"\"\n",
        "    video = io.open(filepath, \"r+b\").read()\n",
        "    encoded = base64.b64encode(video)\n",
        "\n",
        "    display.display(\n",
        "        display.HTML(\n",
        "            data='''\n",
        "            <video alt=\"test\" autoplay  loop controls style=\"height: 400px;\">\n",
        "                <source src=\"data:video/mp4;base64,{0}\" type=\"video/mp4\" />\n",
        "            </video>\n",
        "            '''.format(encoded.decode('ascii'))\n",
        "        )\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i5gDkk5Xn04N"
      },
      "source": [
        "def show_observation_space() -> None:\n",
        "    \"\"\"env環境から算出される値を確認.\n",
        "\n",
        "    Notes:\n",
        "        env.close(), env.reset()が呼ばれるタイミングでMonitorによる保存がなされるようです。\n",
        "    \"\"\"\n",
        "    env = gwrappers.Monitor(gym.make(\"CartPole-v0\"), \"./video\", force=True)\n",
        "    _logger.info(f\"observation space: {env.observation_space}\")\n",
        "    _logger.info(f\"action space: {env.action_space}\")\n",
        "\n",
        "    obs = env.reset()\n",
        "    _logger.info(f\"initial observation: {obs}\")\n",
        "\n",
        "    action = env.action_space.sample()\n",
        "    obs, r, done, info = env.step(action)\n",
        "    _logger.info(f\"next observation: {obs}\")\n",
        "    _logger.info(f\"reward: {r}\")\n",
        "    _logger.info(f\"done: {done}\")\n",
        "    _logger.info(f\"info: {info}\")\n",
        "\n",
        "    while True:\n",
        "        env.render()\n",
        "        action = env.action_space.sample()\n",
        "        obs, r, done, info = env.step(action)\n",
        "        if done:\n",
        "            break\n",
        "    env.close()\n",
        "\n",
        "    for filepath in pathlib.Path(\"video\").glob(\"*.mp4\"):\n",
        "        _logger.info(filepath)\n",
        "        show_video(filepath)\n",
        "\n",
        "\n",
        "show_observation_space()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WHQAGR78rmJc"
      },
      "source": [
        "# DoubleDQN\n",
        "\n",
        "DoubleDQNを利用してQ関数を最適化する。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3mG3iI0tf5ZQ"
      },
      "source": [
        "class QFunction(nn.Module):\n",
        "    def __init__(self, obs_size: int, n_actions: int):\n",
        "        super().__init__()\n",
        "        self.l1 = nn.Linear(obs_size, 50)\n",
        "        self.l2 = nn.Linear(50, 50)\n",
        "        self.l3 = nn.Linear(50, n_actions)\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        h = F.relu(self.l1(x), inplace=True)\n",
        "        h = F.relu(self.l2(h), inplace=True)\n",
        "        h = self.l3(h)\n",
        "\n",
        "        return pfrl.action_value.DiscreteActionValue(h)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wJy7ZbNKg2y-"
      },
      "source": [
        "def create_qfunc(env) -> nn.Module:\n",
        "    obs_size = env.observation_space.low.size\n",
        "    n_actions = env.action_space.n\n",
        "    q_func = QFunction(obs_size, n_actions)\n",
        "\n",
        "    return q_func"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rm13vdG7h2hz"
      },
      "source": [
        "def convert_float32(x: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"torch.Tensorはfloat32なので、変換するための関数.\"\"\"\n",
        "    return x.astype(np.float32, copy=False)\n",
        "\n",
        "\n",
        "def create_agent(env, qfunc: nn.Module):\n",
        "    \"\"\"agentの作成.\"\"\"\n",
        "    optimizer = optim.Adam(qfunc.parameters(), eps=1e-2)\n",
        "    explorer = pfrl.explorers.ConstantEpsilonGreedy(\n",
        "        epsilon=0.3,\n",
        "        random_action_func=env.action_space.sample\n",
        "    )\n",
        "    replay_buffer = pfrl.replay_buffers.ReplayBuffer(capacity=10 ** 6)\n",
        "\n",
        "    agent = pfrl.agents.DoubleDQN(\n",
        "        qfunc,\n",
        "        optimizer,\n",
        "        replay_buffer,\n",
        "        gamma=0.9,\n",
        "        explorer=explorer,\n",
        "        gpu=-1,\n",
        "        replay_start_size=500,\n",
        "        update_interval=1,\n",
        "        target_update_interval=100,\n",
        "        phi=convert_float32,\n",
        "    )\n",
        "\n",
        "    return agent"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_a0_9sHfnG_x"
      },
      "source": [
        "def make_env(seed: int = 0, test: bool = False):\n",
        "    \"\"\"環境を生成して返す.\"\"\"\n",
        "    env = gym.make(\"CartPole-v0\")\n",
        "    env.seed(seed)\n",
        "\n",
        "    outdir = \"mresults\" + (\"_test\" if test else \"_train\")\n",
        "    !rm -r $outdir\n",
        "    mode = \"evaluation\" if test else \"training\"\n",
        "    env = pfrl.wrappers.Monitor(env, outdir, mode=mode)\n",
        "\n",
        "    return env"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-TlcpFM3lr6b"
      },
      "source": [
        "def train_and_evaluation() -> None:\n",
        "    environ = make_env(seed=0, test=False)\n",
        "    environ_eval = make_env(seed=42, test=True)\n",
        "    agent = create_agent(environ, create_qfunc(environ))\n",
        "    \n",
        "    pfrl.experiments.train_agent_with_evaluation(\n",
        "        agent,\n",
        "        environ,\n",
        "        steps=10000,\n",
        "        eval_n_steps=None,\n",
        "        eval_n_episodes=10,\n",
        "        train_max_episode_len=200,\n",
        "        eval_interval=1000,\n",
        "        eval_env=environ_eval,\n",
        "        outdir=\"result\",\n",
        "    )\n",
        "\n",
        "\n",
        "train_and_evaluation()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mj6juU83o7-h"
      },
      "source": [
        "def show_videos(dirname: str) -> None:\n",
        "    \"\"\"結果ディレクトリに保存されたレンダリングされたデータの最初と最後を表示.\"\"\"\n",
        "    files = sorted(list(pathlib.Path(dirname).glob(\"*.mp4\")))\n",
        "\n",
        "    filepath = files[0]\n",
        "    _logger.info(filepath)\n",
        "    show_video(filepath)\n",
        "\n",
        "    filepath = files[-1]\n",
        "    _logger.info(filepath)\n",
        "    show_video(filepath)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FHLcVkLbs91X"
      },
      "source": [
        "# 学習時のデータ\n",
        "show_videos(\"mresults_train\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FZlJ1QVNpeiY"
      },
      "source": [
        "# テストデータ\n",
        "show_videos(\"mresults_test\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n5GlRKSmDLx"
      },
      "source": [
        "# 参考情報: train_agent_with_evaluationで実行される内容\n",
        "\n",
        "PFRLで`pfrl.experiments.train_agent_with_evaluation`を利用した場合に実行される内容を書き出したもの。\n",
        "似たようなことが内部で行われている。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PrvvFOsMhKMI"
      },
      "source": [
        "def train(\n",
        "    env,\n",
        "    agent,\n",
        "    n_episodes: int = 300,\n",
        "    max_episode_len: int = 200,\n",
        ") -> None:\n",
        "    for ep in range(n_episodes):\n",
        "        obs = env.reset()\n",
        "        rewards = 0\n",
        "        time_step = 0\n",
        "        while True:\n",
        "            # env.render()\n",
        "            action = agent.act(obs)\n",
        "            obs, reward, done, _ = env.step(action)\n",
        "            rewards += reward\n",
        "            time_step += 1\n",
        "            reset = time_step == max_episode_len\n",
        "            agent.observe(obs, reward, done, reset)\n",
        "            if done or reset:\n",
        "                break\n",
        "        if (ep + 1) % 10 == 0:\n",
        "            _logger.info(f\"episode: {ep}, rewards: {rewards}\")\n",
        "        if (ep + 1) % 50 == 0:\n",
        "            _logger.info(f\"statistics: {agent.get_statistics()}\")\n",
        "\n",
        "    _logger.info(\"finished\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0v3KIkY7ki1h"
      },
      "source": [
        "def eval(agent, env, max_episode: int = 200) -> None:\n",
        "    with agent.eval_mode():\n",
        "        for ep in range(10):\n",
        "            obs = env.reset()\n",
        "            rewards = 0\n",
        "            time_step = 0\n",
        "            while True:\n",
        "                action = agent.act(obs)\n",
        "                obs, reward, done, _ = env.step(action)\n",
        "                rewards += reward\n",
        "                time_step += 1\n",
        "                reset = time_step == max_episode\n",
        "                agent.observe(obs, reward, done, reset)\n",
        "                if done or reset:\n",
        "                    break\n",
        "            _logger.info(f\"evaluation episode: {ep}, rewards: {rewards}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zMiXKF5zlKtm"
      },
      "source": [
        "def train_and_eval() -> None:\n",
        "    environ = gym.make(\"CartPole-v0\")\n",
        "    agent = create_agent(environ, create_qfunc(environ))\n",
        "    train(\n",
        "        environ,\n",
        "        agent,\n",
        "        n_episodes=300,\n",
        "        max_episode_len=200,\n",
        "    )\n",
        "    eval(agent, environ, max_episode=200)\n",
        "\n",
        "\n",
        "train_and_eval()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}